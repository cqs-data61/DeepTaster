{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15f0443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seonhye/.local/lib/python3.7/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import ToTensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1c4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tiny imagenet\n",
    "def generate_dataloader(data, name, transform):\n",
    "    if data is None: \n",
    "        return None\n",
    "    if transform is None:\n",
    "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
    "    else:\n",
    "        dataset = datasets.ImageFolder(data, transform=transform)\n",
    "    kwargs = {}\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0667c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset\n",
    "def set_dataset(dataset):\n",
    "    torch.manual_seed(107)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    if dataset=='tiny_imagenet':\n",
    "        class_num=100\n",
    "        if not os.path.exists(\"./tiny_imagenet\"):\n",
    "            os.mkdir(\"./tiny_imagenet\")\n",
    "        if not os.path.exists(\"./tiny_imagenet/model\"):\n",
    "            os.mkdir(\"./tiny_imagenet/model\")\n",
    "        DATA_DIR = './tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n",
    "        # Define training and validation data paths\n",
    "        TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "        VALID_DIR = os.path.join(DATA_DIR, 'val')\n",
    "        train_data = generate_dataloader(TRAIN_DIR, \"train\",\n",
    "                                        transform=transform)\n",
    "        test_data=generate_dataloader(VALID_DIR, \"val\",\n",
    "                                        transform=transform)\n",
    "\n",
    "    elif dataset=='cifar10': \n",
    "        class_num=10\n",
    "        if not os.path.exists(\"./cifar10\"):\n",
    "            os.mkdir(\"./cifar10\")\n",
    "        if not os.path.exists(\"./cifar10/model\"):\n",
    "            os.mkdir(\"./cifar10/model\")\n",
    "        train_data = datasets.CIFAR10(\n",
    "        root = 'data',\n",
    "        train = True,                         \n",
    "        transform = transform, \n",
    "        download = True,            \n",
    "        )\n",
    "        test_data = datasets.CIFAR10(\n",
    "            root = 'data', \n",
    "            train = False, \n",
    "            transform = transform\n",
    "        )\n",
    "    elif dataset=='MNIST':\n",
    "        class_num=10\n",
    "        if not os.path.exists(\"./MNIST\"):\n",
    "            os.mkdir(\"./MNIST\")\n",
    "        if not os.path.exists(\"./MNIST/model\"):\n",
    "            os.mkdir(\"./MNIST/model\")\n",
    "        transform = transforms.Compose(\n",
    "        [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        train_data = datasets.MNIST(\n",
    "            root = 'data',\n",
    "            train=True,                      \n",
    "            transform = transform, \n",
    "            download = True,            \n",
    "        )\n",
    "        test_data = datasets.MNIST(\n",
    "            root = 'data',\n",
    "            train=False,                                 \n",
    "            transform = transform, \n",
    "            download = True,            \n",
    "        )\n",
    "    else:\n",
    "        print(\"dataset error\")\n",
    "        exit()\n",
    "    return train_data, test_data, class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477aa107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_architecture(architecture, device, class_num):\n",
    "    # Set model\n",
    "    if architecture=='Resnet18':\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        fc=model.fc\n",
    "        in_dim = fc.in_features\n",
    "        model.fc=nn.Linear(in_dim,class_num)\n",
    "    elif architecture=='Vgg16':\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        num_features = model.classifier[6].in_features\n",
    "        features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features,class_num)]) # Add our layer with 4 outputs\n",
    "        model.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "    elif architecture=='Alexnet':\n",
    "        model = models.alexnet(pretrained=False)\n",
    "        num_features = model.classifier[6].in_features\n",
    "        features = list(model.classifier.children())[:-1] # Remove last layer\n",
    "        features.extend([nn.Linear(num_features,class_num)]) # Add our layer with 4 outputs\n",
    "        model.classifier = nn.Sequential(*features) # Replace the model classifier\n",
    "    elif architecture=='Densenet161':\n",
    "        model = models.densenet161(pretrained=False)\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(num_ftrs, class_num)\n",
    "    else:\n",
    "        print(\"architecture error\")\n",
    "        exit()\n",
    "    model=model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb69a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(input_model, criterion, optimizer, scheduler, device, num_epochs=10):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(input_model.state_dict())\n",
    "    best_acc = 0.0   \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    avg_loss_val = 0\n",
    "    avg_acc_val = 0\n",
    "    \n",
    "    train_batches = len(train_loader)\n",
    "    val_batches = len(test_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        loss_train = 0\n",
    "        loss_val = 0\n",
    "        acc_train = 0\n",
    "        acc_val = 0\n",
    "        \n",
    "        input_model.train(True)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rTraining batch {}/{}\".format(i, train_batches), end='', flush=True)\n",
    "\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = input_model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            acc_train += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avg_loss = loss_train / len(train_loader.dataset)\n",
    "        avg_acc = acc_train / len(train_loader.dataset)\n",
    "        \n",
    "        \n",
    "        print()\n",
    "        \n",
    "        input_model.train(False)\n",
    "        input_model.eval()\n",
    "            \n",
    "        for i, data in enumerate(test_loader):\n",
    "            if i % 100 == 0:\n",
    "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
    "            \n",
    "            inputs, labels = data\n",
    "            \n",
    "\n",
    "            inputs, labels = Variable(inputs.to(device), volatile=True), Variable(labels.to(device), volatile=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = input_model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss_val += loss.item()\n",
    "            acc_val += torch.sum(preds == labels.data)\n",
    "            \n",
    "            del inputs, labels, outputs, preds\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        avg_loss_val = loss_val / len(test_loader.dataset)\n",
    "        avg_acc_val = acc_val / len(test_loader.dataset)\n",
    "        if epoch%5==0 or epoch==opt.epochs:\n",
    "          print()\n",
    "          print(\"Epoch {} result: \".format(epoch))\n",
    "          print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
    "          print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
    "          print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
    "          print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
    "          print('-' * 10)\n",
    "          print()\n",
    "        \n",
    "        if avg_acc_val > best_acc:\n",
    "            best_acc = avg_acc_val\n",
    "            best_model_wts = copy.deepcopy(input_model.state_dict())\n",
    "        \n",
    "    elapsed_time = time.time() - since\n",
    "    print()\n",
    "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
    "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
    "\n",
    "    return input_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380ed2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7facf0089210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f768fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset and architecture\n",
    "\n",
    "# cifar10, tiny_imagenet, MNIST is possible for dataset\n",
    "dataset = \"cifar10\"\n",
    "# Resnet18,Vgg16, Alexnet, Densenet161 is possible for architecture\n",
    "architecture = \"Resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1499e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, class_num = set_dataset(dataset)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                dataset=train_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=test_data,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dc1c76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_architecture(architecture, device,class_num)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if dataset=='cifar10':\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.011, momentum=0.9, weight_decay=5e-4)\n",
    "elif dataset=='MNIST':\n",
    "    if architecture == 'Resnet18' or architecture == 'Vgg16':\n",
    "        optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    else:\n",
    "        optimizer_ft = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "elif dataset=='tiny_imagenet':\n",
    "    optimizer_ft = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe113729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 300/391\n",
      "Validation batch 0/79"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seonhye/.local/lib/python3.7/site-packages/ipykernel_launcher.py:61: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 result: \n",
      "Avg loss (train): 0.0157\n",
      "Avg acc (train): 0.2268\n",
      "Avg loss (val): 0.0130\n",
      "Avg acc (val): 0.3679\n",
      "----------\n",
      "\n",
      "\n",
      "Training completed in 0m 47s\n",
      "Best acc: 0.3679\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, device, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7f2e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_model,  './'+dataset+'/model/'+architecture+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13110ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
